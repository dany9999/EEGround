

# =====================
# TRAINING PARAMETERS
# =====================
epochs: 100
batch_size: 128
num_workers: 20
early_stopping_patience: 10
save_every: 10
resume: false  # pu√≤ essere true per riprendere da checkpoint

# =====================
# OPTIMIZER
# =====================
lr: 0.0001       # learning rate iniziale

encoder_lr: 0.00000996 # learning rate per l'encoder se si fa full finetuning
head_lr: 0.000109     # learning rate per la testa di classificazione


weight_decay: 1e-6

# =====================
# MODEL ARCHITECTURE
# =====================
emb_size: 256
heads: 8
depth: 4
n_channels: 18
n_classes: 1
n_fft: 250
hop_length: 125

# =====================
# LOSS FUNCTION
# =====================
criterion_name: "focal"   # "bce" o "focal"
focal_alpha: 0.75
focal_gamma: 2.0
threshold: 0.5


# =====================
# FINETUNING SETTINGS
# =====================
finetune_mode: "full_finetune"  # "from_scratch", "full_finetune", "frozen_encoder"


# =====================
# DATASET PATHS
# =====================
dataset_path_4s: "../../Datasets/Bipolar/chb_mit/raw"
dataset_path_8s: "../../Datasets/Bipolar/chb_mit/8sec"
gt_path: "../../Datasets/chb_mit/GT"  # utile se la pipeline la richiede
pretrain_model_path: "logs/pretrain-new-mask_stft/best_encoder_only.pt"

# =====================
# OUTPUT DIRECTORIES
# =====================
save_dir: "./checkpoints_finetuning"
log_dir: "log_full_finetuning_perc_norm_CHB-MIT_new_mask"

# =====================
# OPTUNA SETTINGS
# =====================

seed: 42
study_name: "BIOT_finetune_study"
optuna_storage: "sqlite:///optuna_biot.db"