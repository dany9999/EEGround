

# =====================
# TRAINING PARAMETERS
# =====================
epochs: 100
batch_size: 128
num_workers: 20
early_stopping_patience: 15
save_every: 10
resume: false  # pu√≤ essere true per riprendere da checkpoint

# =====================
# OPTIMIZER
# =====================
lr: 0.00005          # learning rate iniziale
weight_decay: 1e-6

# =====================
# MODEL ARCHITECTURE
# =====================
emb_size: 256
heads: 8
depth: 4
n_channels: 16
n_classes: 1
n_fft: 200
hop_length: 100

# =====================
# LOSS FUNCTION
# =====================
criterion_name: "focal"   # "bce" o "focal"
focal_alpha: 0.25
focal_gamma: 1.7
threshold: 0.5
use_pos_weight: false     # se vuoi usare BCE bilanciata in futuro

# =====================
# FINETUNING SETTINGS
# =====================
finetune_mode: "from_scratch"  # "from_scratch", "full_finetune", "frozen_encoder"
pretrained_ckpt: "BIOT_vanilla/pretrained_models/EEG-PREST-16-channels.ckpt"

# =====================
# DATASET PATHS
# =====================
dataset_path_4s: "../../Datasets/Bipolar/chb_mit/bipolar_data"
dataset_path_8s: "../../Datasets/Bipolar/chb_mit/8sec"
gt_path: "../../Datasets/chb_mit/GT"  # utile se la pipeline la richiede

# =====================
# OUTPUT DIRECTORIES
# =====================
save_dir: "./checkpoints_finetuning"
log_dir: "./logs/finetuning_from_scratch"

# =====================
# OPTUNA SETTINGS
# =====================

seed: 42
study_name: "BIOT_finetune_study"
optuna_storage: "sqlite:///optuna_biot.db"