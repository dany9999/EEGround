{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c69c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def leave_one_out_splits(patients, val_count=2):\n",
    "    splits = []\n",
    "    for i, test_patient in enumerate(patients):\n",
    "        remaining = [p for j, p in enumerate(patients) if j != i]\n",
    "        val_combinations = list(combinations(remaining, val_count))\n",
    "        for val_patients in val_combinations:\n",
    "            train_patients = [p for p in remaining if p not in val_patients]\n",
    "            splits.append({\n",
    "                \"train\": train_patients,\n",
    "                \"val\": list(val_patients),\n",
    "                \"test\": [test_patient]\n",
    "            })\n",
    "            break  # ← prendi solo la prima combinazione di validation\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b26f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from scipy.signal import resample\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CHBMITLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, files, sampling_rate=200):\n",
    "        self.root = root\n",
    "        self.files = files\n",
    "        self.default_rate = 256\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = pickle.load(open(os.path.join(self.root, self.files[index]), \"rb\"))\n",
    "        X = sample[\"X\"]\n",
    "        # 2560 -> 2000, from 256Hz to ?\n",
    "        if self.sampling_rate != self.default_rate:\n",
    "            X = resample(X, 10 * self.sampling_rate, axis=-1)\n",
    "        \n",
    "\n",
    "        X = X / (\n",
    "            np.quantile(np.abs(X), q=0.95, method=\"linear\", axis=-1, keepdims=True)\n",
    "            + 1e-8\n",
    "        )\n",
    "        Y = sample[\"y\"]\n",
    "        X = torch.FloatTensor(X)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b774f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'train': [], 'val': ['train', 'val'], 'test': ['test']}, {'train': [], 'val': ['test', 'val'], 'test': ['train']}, {'train': [], 'val': ['test', 'train'], 'test': ['val']}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_patients = sorted(os.listdir(\"CHB-MIT/clean_segments\"))\n",
    "\n",
    "splits = leave_one_out_splits(all_patients, val_count=2)\n",
    "\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00e73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def make_loader(patients_list, split, config):\n",
    "    segment_files = []\n",
    "    for patient in patients_list:\n",
    "        path = os.path.join(\"CHB-MIT/clean_segments\", patient, split)\n",
    "        if os.path.exists(path):\n",
    "            files = [os.path.join(patient, split, f) for f in os.listdir(path)]\n",
    "            segment_files.extend(files)\n",
    "    dataset = CHBMITLoader(\"CHB-MIT/clean_segments\", segment_files, config[\"sampling_rate\"])\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=(split == \"train\"),\n",
    "        drop_last=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        persistent_workers=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3b646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82291218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from model import LitModel_finetune  # Assuming LitModel_finetune is defined in model.py\n",
    "def supervised(config, train_loader, val_loader, test_loader, iteration_idx):\n",
    "    lightning_model = LitModel_finetune(config)\n",
    "    os.makedirs(\"log-finetuning\", exist_ok=True)\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=\"log-finetuning\", name=f\"run-{iteration_idx}\")\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_auroc\", patience=5, mode=\"max\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"log-finetuning/run-{iteration_idx}/checkpoints\",\n",
    "        filename=\"model-{epoch:02d}-{val_loss:.4f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=3,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        max_epochs=config[\"epochs\"],\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    trainer.fit(lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    result = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")[0]\n",
    "    print(f\"=== Split {iteration_idx} Result ===\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11307d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = load_config(\"configs/finetuning.yml\")\n",
    "    all_patients = sorted(os.listdir(\"CHB-MIT/clean_segments\"))\n",
    "\n",
    "    splits = leave_one_out_splits(all_patients, val_count=2)\n",
    "\n",
    "    for idx, split in enumerate(splits):\n",
    "        print(f\"\\n--- Running Split {idx + 1}/{len(splits)} ---\")\n",
    "        train_loader = make_loader(split[\"train\"], \"train\", config)\n",
    "        val_loader = make_loader(split[\"val\"], \"val\", config)\n",
    "        test_loader = make_loader(split[\"test\"], \"test\", config)\n",
    "        supervised(config, train_loader, val_loader, test_loader, idx + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
